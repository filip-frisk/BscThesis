{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["Find all differences here https://www.diffchecker.com/PlnW76qN<br>\n", "imports"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import argparse\n", "import os\n", "import random\n", "import shutil\n", "import time\n", "import warnings\n", "import PIL\n", "import torch\n", "import torch.nn as nn\n", "import torch.nn.parallel\n", "import torch.backends.cudnn as cudnn\n", "import torch.distributed as dist\n", "import torch.optim\n", "import torch.multiprocessing as mp\n", "import torch.utils.data\n", "import torch.utils.data.distributed\n", "import torchvision.transforms as transforms\n", "import torchvision.datasets as datasets\n", "import torchvision.models as models\n", "from sklearn.metrics import classification_report\n", "from sklearn.metrics import confusion_matrix\n", "from efficientnet_pytorch import EfficientNet\n", "from CustomDataset import CustomDataset\n", "import matplotlib.pyplot as plt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: run_model<br>\n", "Function: Handles if user specified seed or GPU and warns the user of consequences (process slowdown)<br>\n", "Input: loaders (dict with three dataLoader objects), split (current split number in k crossfold validation), args (arguments from main) and class_names<br>\n", "Output: main_worker with same input<br>\n", "Source: https://github.com/pytorch/examples/blob/master/imagenet/main.py<br>\n", "Source2: https://www.xilinx.com/html_docs/vitis_ai/1_3/pytorch_ex.html<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["num_classes = 4\n", "best_all_splits_acc = 0\n", "def run_model(loaders, split, args, class_names,num_item_per_class):\n", "    # Uses seed if provided\n", "    if args.seed is not None:\n", "        random.seed(args.seed)\n", "        torch.manual_seed(args.seed)\n", "        cudnn.deterministic = True\n", "        warnings.warn('You have chosen to seed training. '\n", "                      'This will turn on the CUDNN deterministic setting, '\n", "                      'which can slow down your training considerably! '\n", "                      'You may see unexpected behavior when restarting '\n", "                      'from checkpoints.')\n\n", "    # Warns user if they chose specific GPU, and prints out number of available GPUs\n", "    if args.gpu is not None:\n", "        warnings.warn('You have chosen a specific GPU. This will completely '\n", "                      'disable data parallelism.')\n", "    ngpus = torch.cuda.device_count()\n", "    print(\"Available GPUs: {}\".format(ngpus))\n", "    return main_worker(loaders, split, args.gpu, ngpus, args, num_item_per_class)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Type: function<br>\n", "Name: main_worker<br>\n", "Function: After run_model we have handled some manual config (seed or GPU), this function creates a model according to<br>\n", "        passed arguments and trains/validates/tests it on the current data in the dataloader.<br>\n", "        This function drives the whole training, validation and testing process.<br>\n", "Input: Data loader (dictionary), current split (k), gpu, number of gpus, args, number of items in each class<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def main_worker(loaders, split, gpu, ngpus, args, num_item_per_class):\n", "    global best_all_splits_acc\n", "    best_acc1 = 0\n", "    args.gpu = gpu\n", "    output_destination = args.outdest\n", "    if args.gpu is not None:\n", "        print(\"Use GPU: {} for training\".format(args.gpu))\n\n", "    # Creates model using EfficientNet library\n", "    if 'efficientnet-b' in args.arch:  # NEW\n", "        if args.pretrained:\n", "            model = EfficientNet.from_pretrained(args.arch, advprop=args.advprop, num_classes=num_classes)\n", "            print(\"=> using pre-trained model '{}'\".format(args.arch))\n", "        else:\n", "            print(\"=> creating model '{}'\".format(args.arch))\n", "            model = EfficientNet.from_name(args.arch, override_params={'num_classes': num_classes})\n", "    else:\n", "        print(\"Only EfficientNet models are supported.\")\n", "        quit()\n\n", "    # Handles the case where the user has specified GPU, to set the device manually\n", "    if args.gpu is not None:\n", "        torch.cuda.set_device(args.gpu)\n", "        model = model.cuda(args.gpu)\n", "    else:\n", "        # If GPU not specifiec check if CUDA is available, else use CPU\n", "        if torch.cuda.is_available():\n", "            model = torch.nn.DataParallel(model).cuda()\n", "        else:\n", "            model.to(\"cpu\")\n\n", "    # Class weights = (total_count - class_count) / total_count\n", "    # old weights:\n", "    # weights = [25137/2017, 25137/3211, 25137/7296, 25137/12519]\n", "    # weights = [(25137-2017)/25137, (25137-3211)/25137, (25137-7296)/25137, (25137-12519)/25137]\n", "    # Our changes:\n", "    total_num_train = sum(num_item_per_class)\n", "    weights = [(total_num_train-num)/total_num_train for num in num_item_per_class]\n", "    class_weights = torch.FloatTensor(weights)\n", "    if torch.cuda.is_available():\n", "        class_weights = class_weights.cuda()\n\n", "    # Define loss function (criterion)\n", "    # Cross Entropy loss is a combination of LogSoftMax and NLLLoss i one single class\n", "    # changes made to if statement structure\n", "    if torch.cuda.is_available():\n", "        if args.upsample:\n", "            criterion = nn.CrossEntropyLoss().cuda(args.gpu)\n", "        else:\n", "            criterion = nn.CrossEntropyLoss(weight=class_weights).cuda(args.gpu)\n", "    else:\n", "        if args.upsample:\n", "            criterion = nn.CrossEntropyLoss()\n", "        else:\n", "            criterion = nn.CrossEntropyLoss(weight=class_weights)\n\n", "    # Defines Stocastic Gradient Descent Optimizer\n", "    optimizer = torch.optim.SGD(model.parameters(), args.lr,\n", "                                nesterov=True,\n", "                                momentum=args.momentum,\n", "                                weight_decay=args.weight_decay)\n", "    # optimizer = torch.optim.AdamW(model.parameters(), args.lr,weight_decay=args.weight_decay, amsgrad=True)\n\n", "    # Resume from a checkpoint is args.resume is not an empty string and a valid path\n", "    if args.resume:\n", "        if os.path.isfile(args.resume):\n", "            print(\"=> loading checkpoint '{}'\".format(args.resume))\n", "            checkpoint = torch.load(args.resume)\n", "            args.start_epoch = checkpoint['epoch']\n", "            best_acc1 = checkpoint['best_acc1']\n", "            if args.gpu is not None:\n", "                # best_acc1 may be from a checkpoint from a different GPU\n", "                best_acc1 = best_acc1.to(args.gpu)\n", "            model.load_state_dict(checkpoint['state_dict'])\n", "            optimizer.load_state_dict(checkpoint['optimizer'])\n", "            print(\"=> loaded checkpoint '{}' (epoch {})\"\n", "                  .format(args.resume, checkpoint['epoch']))\n", "        else:\n", "            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n\n", "    # Eenables the inbuilt cudnn auto-tuner to find the best algorithm to use for the hardware\n", "    cudnn.benchmark = True\n\n", "    # Unfreezes only the classification layer if feature exract argument is set to True\n", "    # param.requires_grad = True -> unfreezed\n", "    # param.requires_grad = False -> freezed\n", "    if args.feature_extract:\n", "        c = 0\n", "        for param in model.parameters():\n", "            c+= 1\n", "            if c < 213:\n", "                param.requires_grad = False\n\n", "    # Defines losses and accuracy to later be used for plotting the loss and accuracy curves\n", "    epochs = [epoch for epoch in range(args.start_epoch, args.epochs)]\n", "    losses = []\n", "    accs = []\n\n", "    # Trains and validate in all epochs\n", "    for epoch in epochs:\n", "        adjust_learning_rate(optimizer, epoch, args.lr)\n\n", "        # train for one epoch\n", "        train(loaders['train'], model, criterion, optimizer, epoch, args, losses, accs)\n\n", "        # evaluate on validation set\n", "        acc1, val_classification, val_confusion_matrix = validate(loaders['val'], model, criterion, args,'Validation: ')\n\n", "        # remember best acc@1\n", "        is_best = acc1 > best_acc1\n", "        best_acc1 = max(acc1, best_acc1)\n\n", "        # remember best acc@1 over all k splits\n", "        is_best_all = acc1 > best_all_splits_acc\n", "        best_all_splits_acc = max(acc1, best_all_splits_acc)\n\n", "        # Saves checkpoint of current model see function comment\n", "        save_checkpoint({\n", "            'epoch': epoch + 1,# if we use resume (args.resume) we should start from the next epoch\n", "            'arch': args.arch,\n", "            'state_dict': model.state_dict(),\n", "            'best_acc1': best_acc1,\n", "            'optimizer': optimizer.state_dict(),\n", "        }, is_best, is_best_all, split, output_destination)\n\n", "        # Classifies current validation set and writes statistics to correct file\n", "        if args.validate:\n", "            if is_best:\n", "                with open(output_destination + '/res_val_{}.txt'.format(split), 'w') as f:\n", "                    print(\"epoch:\", epoch, file = f)\n", "                    print(acc1, file=f)\n", "                    print(val_classification, file=f)\n", "                    print(val_confusion_matrix, file=f)\n", "            if is_best_all:\n", "                with open(output_destination + '/res_val.txt', 'w') as f:\n", "                    print(\"split:\", split, file = f)\n", "                    print(\"epoch:\", epoch, file = f)\n", "                    print(acc1, file=f)\n", "                    print(val_classification, file=f)\n", "                    print(val_confusion_matrix, file=f)\n\n", "        # Classifies testing set and writes statistics to correct file\n", "        if args.evaluate:\n", "            if is_best:\n", "                test_acc1, test_classification, test_confusion_matrix = validate(loaders['test'], model, criterion, args, 'Test: ')\n", "                with open(output_destination + '/res_test_{}.txt'.format(split), 'w') as f:\n", "                    print(\"epoch:\", epoch, file=f)\n", "                    print(test_acc1, file=f)\n", "                    print(test_classification, file=f)\n", "                    print(test_confusion_matrix, file=f)\n", "            if is_best_all:\n", "                with open(output_destination + '/res_test.txt', 'w') as f:\n", "                    print(\"split:\", split, file = f)\n", "                    print(\"epoch:\", epoch, file = f)\n", "                    print(test_acc1, file=f)\n", "                    print(test_classification, file=f)\n", "                    print(test_confusion_matrix, file=f)\n", "    with open(output_destination + '/losses_vs_epochs_{}.txt'.format(split), 'w') as f:\n", "        print(losses, file = f)\n", "        print(epochs, file = f)\n", "    with open(output_destination + '/acc_vs_epochs_{}.txt'.format(split), 'w') as f:\n", "        print(accs, file = f)\n", "        print(epochs, file = f)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: train<br>\n", "Function: trains the model with the given loader, model, criterion and optimizer<br>\n", "Output: Creates print log in terminal for progress and average for<br>\n", "Computational time, Data loading time,Loss, Acc@1.<br>\n", "Epoch: [epoch number] [Bacth count/Number of batches in epoch]<br>\n", "Each epoch consists of a fixed number of batches depending on args.batch_size parameter<br>\n", "Number of batxhes in epoch = roof(Dataset size (labels) / batch size)<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(train_loader, model, criterion, optimizer, epoch, args, losses_vec, accs_vec):\n", "    # Epoch and batch count / Time per batch / Data loading time for batch\n", "    batch_time = AverageMeter('Time', ':6.3f') # Training time per batch\n", "    data_time = AverageMeter('Data loading time', ':6.3f') # Data loading time from python iteration computation\n", "    losses = AverageMeter('Loss', ':.4e')\n", "    top1 = AverageMeter('Acc@1', ':6.2f')\n", "    progress = ProgressMeter(len(train_loader), batch_time, data_time, losses, top1,\n", "                              prefix=\"Epoch: [{}]\".format(epoch))\n\n", "    # Switches to train mode for the EfficientNet\n", "    model.train()\n", "    end = time.time()\n", "    running_loss = 0.0\n", "    for i, (images, target) in enumerate(train_loader):\n", "        # measure data loading time\n", "        data_time.update(time.time() - end)\n", "        if args.gpu is not None:\n", "            images = images.cuda(args.gpu, non_blocking=True)\n", "        if torch.cuda.is_available():\n", "            target = target.cuda(args.gpu, non_blocking=True)\n\n", "        # compute output\n", "        output = model(images)\n", "        loss = criterion(output, target)\n\n", "        # measure accuracy and record loss\n", "        acc1 = accuracy(output, target, topk=(1,))\n", "        losses.update(loss.item(), images.size(0))\n", "        top1.update(acc1[0].item(), images.size(0))\n\n", "        # compute gradient and do SGD step\n", "        optimizer.zero_grad()\n", "        loss.backward()\n", "        optimizer.step()\n\n", "        # measure elapsed time\n", "        batch_time.update(time.time() - end)\n", "        end = time.time()\n", "        if i % args.print_freq == 0:\n", "            progress.print(i)\n", "    losses_vec.append(losses.avg)\n", "    accs_vec.append(top1.avg)\n", "\"\"\"\n", "Name: validate\n", "Function: validate or tests the model with the given loader, model, criterion\n", "Output: Creates print log in terminal for progress and average for\n", "Computational time, Data loading time,Loss, Acc@1.\n", "Acc@1 is top 1% accuracy which is the highest accuracy of 4 in the lat classification layer.\n", "For example [0.3,0.2,0.4,0.1] gives class 3  as top 1 class (with aprediction ccuracy 0.4),\n", "this can also be calculatied by taking diagonal elements i confusion matrix divvided by total #\n", "of elements. Acc C1 is the number of data points we predicted correctly from class 1 in data set.\n", "Source: https://github.com/DingXiaoH/RepVGG/blob/main/train.py\n", "\"\"\"\n", "def validate(val_loader, model, criterion, args, testing_type):\n", "    batch_time = AverageMeter('Time', ':6.3f')\n", "    losses = AverageMeter('Loss', ':.4e')\n", "    top1 = AverageMeter('Acc@1', ':6.2f')\n", "    topC1 = AverageMeter('Acc C1', ':6.2f')\n", "    topC2 = AverageMeter('Acc C2', ':6.2f')\n", "    topC3 = AverageMeter('Acc C3', ':6.2f')\n", "    topC4 = AverageMeter('Acc C4', ':6.2f')\n", "    progress = ProgressMeter(len(val_loader), batch_time, losses, top1, topC1, topC2, topC3, topC4,\n", "                             prefix=testing_type)\n\n", "    # switch to evaluate mode\n", "    model.eval()\n\n", "    #Creates arrays for prediciton values\n", "    y_true, y_pred = [], []\n", "    with torch.no_grad(): # skips the gradient calculation over the weights, not changing any weight in the specified layers\n", "        end = time.time()\n", "        for i, (images, target) in enumerate(val_loader):\n", "            if args.gpu is not None:\n", "                images = images.cuda(args.gpu, non_blocking=True)\n", "            if torch.cuda.is_available():\n", "                target = target.cuda(args.gpu, non_blocking=True)\n\n", "            # compute output (training)\n", "            output = model(images)\n", "            loss = criterion(output, target)\n\n", "            # measure accuracy and record loss\n", "            acc1 = accuracy(output, target, topk=(1,))\n", "            losses.update(loss.item(), images.size(0))\n", "            top1.update(acc1[0].item(), images.size(0))\n\n", "            # gets top 1 prediction\n", "            _, pred = output.topk(1, 1, True, True)\n", "            pred = pred.t()\n", "            y_true.extend(target.detach().cpu().numpy())\n", "            y_pred.extend(pred.detach().cpu().numpy()[0])\n", "            C1indices = [index for index, element in enumerate(target) if element == 0]\n", "            if len(C1indices) > 0:\n", "                accC1 = accuracy(output[C1indices], target[C1indices], topk=(1,))\n", "                topC1.update(accC1[0].item(), len(C1indices))\n", "            C2indices = [index for index, element in enumerate(target) if element == 1]\n", "            if len(C2indices) > 0:\n", "                accC2 = accuracy(output[C2indices], target[C2indices], topk=(1,))\n", "                topC2.update(accC2[0].item(), len(C2indices))\n", "            C3indices = [index for index, element in enumerate(target) if element == 2]\n", "            if len(C3indices) > 0:\n", "                accC3 = accuracy(output[C3indices], target[C3indices], topk=(1,))\n", "                topC3.update(accC3[0].item(), len(C3indices))\n", "            C4indices = [index for index, element in enumerate(target) if element == 3]\n", "            if len(C4indices) > 0:\n", "                accC4 = accuracy(output[C4indices], target[C4indices], topk=(1,))\n", "                topC4.update(accC4[0].item(), len(C4indices))\n\n", "            # measure elapsed time\n", "            batch_time.update(time.time() - end)\n", "            end = time.time()\n", "            if i % args.print_freq == 0:\n", "                progress.print(i)\n", "        print(' * Acc {top1.avg:.3f}'\n", "              .format(top1=top1))\n", "    report = classification_report(y_true, y_pred)\n", "    print(\"classification report:\")\n", "    print(report)\n", "    confusion_matrix = confusion_matix_string(y_true, y_pred)\n", "    print(confusion_matrix)\n", "    return top1.avg, report, confusion_matrix\n", "\"\"\"\n", "Name: save_checkpoint\n", "Function: Saves a checkpoint of all model params in the file 'checkpoint.pth.tar' located in the destination folder.\n", "Inculdes epoch, optimizer, architecture, statedict, best_acc1, optimizer\n", "Input: Model parameters (state), is_best (boolean value), destinantion (PATH)\n", "Output: Saves checkpoint as checkpoint.pth.tar', if is_best = True copies it to the right destination with name model_best_{k value in crossvalidation}.pth.tar\n", "Source: https://github.com/pytorch/examples/blob/master/imagenet/main.py\n", "\"\"\"\n", "def save_checkpoint(state, is_best, is_best_all, split, destination ,filename='/checkpoint.pth.tar'):\n", "    torch.save(state, destination + filename)\n", "    if is_best:\n", "        shutil.copyfile(destination + filename, destination + '/model_best_{}.pth.tar'.format(split))\n", "    if is_best_all:\n", "        shutil.copyfile(destination + filename, destination + '/model_best.pth.tar')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: AverageMeter<br>\n", "Function: Creates a custom string (__str__) with specified name and format and computes and stores the average and current value<br>\n", "Inastance variables: Name of stored value and print format. Also, calculated value, avarage, sum and count.<br>\n", "Source: https://github.com/pytorch/examples/blob/master/imagenet/main.py<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class AverageMeter(object):\n", "    def __init__(self, name, fmt=':f'): # fmt means format, and standard for formatting can be found https://docs.python.org/3/library/string.html#format-specification-mini-language\n", "        self.name = name\n", "        self.fmt = fmt\n", "        self.reset()\n", "    def reset(self):\n", "        self.val = 0\n", "        self.avg = 0\n", "        self.sum = 0\n", "        self.count = 0\n", "    def update(self, val, n=1):\n", "        self.val = val\n", "        self.sum += val * n\n", "        self.count += n\n", "        self.avg = self.sum / self.count\n", "    def __str__(self):\n", "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n", "        return fmtstr.format(**self.__dict__)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: ProgressMeter<br>\n", "Function: Creates a custom string (get_batch_fmtstr) with specified name and format, and given instance variables from AverageMeter objects<br>\n", "Instance variables: Custumized string from AvarageMeter objects, name of object and it's custom format<br>\n", "Source: https://github.com/pytorch/examples/blob/master/imagenet/main.py<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class ProgressMeter(object):\n", "    def __init__(self, num_batches, *meters, prefix=\"\"):\n", "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n", "        self.meters = meters\n", "        self.prefix = prefix\n", "    def print(self, batch):\n", "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n", "        entries += [str(meter) for meter in self.meters]\n", "        print('\\t'.join(entries))\n", "    def _get_batch_fmtstr(self, num_batches):\n", "        num_digits = len(str(num_batches // 1))\n", "        fmt = '{:' + str(num_digits) + 'd}'\n", "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: adjust_learning_rate<br>\n", "Function: Sets the learning rate to the initial learning rate decayed by 3% every 2.4 epochs (assuming we learn more in the beginning)<br>\n", "Input: Optimizer, epoch number and specified learning rate (args.lr)<br>\n", "Output: Updating learning rate in model for next epoch<br>\n", "Source: https://gist.github.com/zachguo/10296432<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def adjust_learning_rate(optimizer, epoch, input_learning_rate):\n", "    learning_rate = input_learning_rate * (0.97 ** (epoch // 2.4))\n", "    for param_group in optimizer.param_groups:\n", "        param_group['lr'] = learning_rate"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: accuracy<br>\n", "Function: Computes the accuracy over the k top predictions for the specified values of k\"<br>\n", "Input: predicted and target and tuple containing wanted accuracies<br>\n", "Output: Calculate accuracy for print log for each epoch printed<br>\n", "Source: https://github.com/bearpaw/pytorch-classification/blob/cc9106d598ff1fe375cc030873ceacfea0499d77/utils/eval.py<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def accuracy(output, target, topk=(1,)):\n", "    with torch.no_grad():\n", "        maxk = max(topk)\n", "        batch_size = target.size(0)\n", "        _, pred = output.topk(maxk, 1, True, True)\n", "        pred = pred.t()\n", "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n", "        res = []\n", "        for k in topk:\n", "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n", "            res.append(correct_k.mul_(100.0 / batch_size))\n", "        return res"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n<br>\n", "Name: print_cm<br>\n", "Function: Prints confusion matrix in terminal (labels are hardcoded)<br>\n", "Input: True and predicted labels<br>\n", "Output: print in  terminal<br>\n", "Source: https://gist.github.com/zachguo/10296432<br>\n", ""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def confusion_matix_string(y_true, y_pred, labels= [0,1,2,3],hide_zeroes=False, hide_diagonal=False, hide_threshold=None):\n", "    cm_string = \"confusion matrix:\\n\"\n", "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n", "    \"\"\"pretty print for confusion matrixes\"\"\"\n", "    columnwidth = max([len(str(x)) for x in labels] + [5])  # 5 is value length\n", "    empty_cell = \" \" * columnwidth\n", "    cm_string += empty_cell + \" t\\p \"\n", "    for label in labels:\n", "        cm_string += \"%{0}s \".format(columnwidth) % label\n", "    cm_string += \"\\n\"\n", "    for i, label1 in enumerate(labels):\n", "        cm_string += \"   %{0}s  \".format(columnwidth) % label1\n", "        for j in range(len(labels)):\n", "            cell = \"%{0}.0f \".format(columnwidth) % cm[i, j]\n", "            if hide_zeroes:\n", "                cell = cell if float(cm[i, j]) != 0 else empty_cell\n", "            if hide_diagonal:\n", "                cell = cell if i != j else empty_cell\n", "            if hide_threshold:\n", "                cell = cell if cm[i, j] > hide_threshold else empty_cell\n", "            cm_string += cell\n", "        cm_string += \"\\n\"\n", "    return cm_string"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["if __name__ == '__main__':\n", "    main()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}